# ë°ì´í„° ì¶”ì¶œ ë„êµ¬ ì‚¬ìš©ë²•

ì´ í´ë”ëŠ” ë°°ë‹¬ì˜ë¯¼ì¡± ë¹„ì •í˜• ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„í•˜ëŠ” ë„êµ¬ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
data_extraction/
â”œâ”€â”€ requirements.txt          # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ data_extractor.py        # ë©”ì¸ ë°ì´í„° ì¶”ì¶œ í´ë˜ìŠ¤
â”œâ”€â”€ text_analyzer.py         # í…ìŠ¤íŠ¸ ë¶„ì„ ë„êµ¬
â”œâ”€â”€ run_extraction.py        # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md               # ì´ íŒŒì¼
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (Windows)
python -m venv delivery_env
delivery_env\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  ë¶„ì„ í¬í•¨)
python run_extraction.py

# í…ìŠ¤íŠ¸ ë¶„ì„ ê±´ë„ˆë›°ê¸°
python run_extraction.py --skip-text-analysis

# ë¡œê·¸ ë¶„ì„ ê±´ë„ˆë›°ê¸°
python run_extraction.py --skip-log-analysis

# Redis ì„¤ì • ë³€ê²½
python run_extraction.py --redis-host localhost --redis-port 6379
```

## ğŸ“Š ì£¼ìš” ê¸°ëŠ¥

### DataExtractor í´ë˜ìŠ¤

- **CSV ë°ì´í„° ì¶”ì¶œ**: `extract_csv_data()`
- **JSON ë°ì´í„° ì¶”ì¶œ**: `extract_json_data()`
- **XML ë°ì´í„° ì¶”ì¶œ**: `extract_xml_data()`
- **ë¡œê·¸ ë°ì´í„° ì¶”ì¶œ**: `extract_log_data()`
- **Pickle ë°ì´í„° ì¶”ì¶œ**: `extract_pickle_data()`
- **Redis ì €ì¥**: `save_to_redis()`
- **ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥**: `save_to_database()`

### TextAnalyzer í´ë˜ìŠ¤

- **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: `preprocess_text()`
- **í† í°í™”**: `tokenize_text()`
- **ê°ì • ë¶„ì„**: `analyze_sentiment()`
- **í‚¤ì›Œë“œ ì¶”ì¶œ**: `extract_keywords()`
- **ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„**: `analyze_message_patterns()`
- **ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±**: `generate_wordcloud()`
- **ê°œì²´ëª… ì¶”ì¶œ**: `extract_entities()`

## ğŸ“ˆ ì¶œë ¥ ê²°ê³¼

ì‹¤í–‰ í›„ ë‹¤ìŒ í´ë”ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
extracted_data/           # ì¶”ì¶œëœ ë°ì´í„°
â”œâ”€â”€ extracted_*.json     # ê° íŒŒì¼ë³„ ì¶”ì¶œ ê²°ê³¼
â””â”€â”€ extraction_summary.json

reports/                 # ë¶„ì„ ë¦¬í¬íŠ¸
â”œâ”€â”€ text_analysis.json
â”œâ”€â”€ text_analysis_report.txt
â”œâ”€â”€ log_analysis.json
â”œâ”€â”€ final_report.json
â””â”€â”€ final_report.txt

visualizations/          # ì‹œê°í™” ê²°ê³¼
â””â”€â”€ wordcloud.png       # ì›Œë“œí´ë¼ìš°ë“œ
```

## ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ

```python
from data_extractor import DataExtractor

# ì´ˆê¸°í™”
extractor = DataExtractor()

# Redis ì—°ê²°
extractor.setup_redis()

# ëª¨ë“  ë°ì´í„° ì¶”ì¶œ
results = extractor.extract_all_data()

# ê²°ê³¼ í™•ì¸
for filename, result in results.items():
    print(f"{filename}: {result['type']}")
```

### í…ìŠ¤íŠ¸ ë¶„ì„

```python
from text_analyzer import TextAnalyzer

# ì´ˆê¸°í™”
analyzer = TextAnalyzer(language='korean')

# í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
texts = ["ë§›ìˆëŠ” ì¹˜í‚¨ ë°°ë‹¬í•´ì£¼ì„¸ìš”!", "í”¼ì ì£¼ë¬¸í–ˆëŠ”ë° ëŠ¦ê²Œ ì™”ì–´ìš”"]

# í‚¤ì›Œë“œ ì¶”ì¶œ
keywords = analyzer.extract_keywords(texts, top_n=10)
print("ì£¼ìš” í‚¤ì›Œë“œ:", keywords)

# ê°ì • ë¶„ì„
for text in texts:
    sentiment = analyzer.analyze_sentiment(text)
    print(f"'{text}': {sentiment['polarity']:.3f}")
```

## âš™ï¸ ì„¤ì • ì˜µì…˜

### Redis ì„¤ì •

```python
extractor = DataExtractor()
extractor.setup_redis(host='localhost', port=6379, db=0)
```

### ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

```python
extractor.setup_database("postgresql://user:password@localhost/dbname")
```

### ì–¸ì–´ ì„¤ì •

```python
# í•œêµ­ì–´
analyzer = TextAnalyzer(language='korean')

# ì˜ì–´
analyzer = TextAnalyzer(language='english')
```

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. Redis ì—°ê²° ì‹¤íŒ¨

```bash
# Redis ì„œë²„ ì‹œì‘ (Windows)
redis-server

# ë˜ëŠ” Docker ì‚¬ìš©
docker run -d -p 6379:6379 redis:latest
```

### 2. í°íŠ¸ ë¬¸ì œ (ì›Œë“œí´ë¼ìš°ë“œ)

Windowsì—ì„œ í•œêµ­ì–´ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹œ í°íŠ¸ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”:

```python
# text_analyzer.pyì˜ generate_wordcloud ë©”ì„œë“œì—ì„œ
font_path = 'C:/Windows/Fonts/malgun.ttf'  # Windows
```

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±

ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
def process_large_file(filename):
    chunk_size = 1000
    for chunk in pd.read_csv(filename, chunksize=chunk_size):
        # ì²­í¬ë³„ ì²˜ë¦¬
        process_chunk(chunk)
```

## ğŸ“‹ ì§€ì› íŒŒì¼ í˜•ì‹

- **CSV**: `.csv` íŒŒì¼
- **JSON**: `.json` íŒŒì¼
- **XML**: `.xml` íŒŒì¼
- **ë¡œê·¸**: `.log` íŒŒì¼
- **Pickle**: `.pickle` íŒŒì¼

## ğŸ” ë¶„ì„ ê¸°ëŠ¥

### í…ìŠ¤íŠ¸ ë¶„ì„
- í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„
- ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •)
- ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„
- ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
- ê°œì²´ëª… ì¶”ì¶œ (ìŒì‹ì , ìŒì‹, ê°€ê²© ë“±)

### ë¡œê·¸ ë¶„ì„
- ë¡œê·¸ ë ˆë²¨ë³„ ë¶„í¬
- ì‹œê°„ëŒ€ë³„ íŒ¨í„´
- ì—ëŸ¬ ë©”ì‹œì§€ ìˆ˜ì§‘
- ì‘ë‹µ ì‹œê°„ ë¶„ì„

### ë°ì´í„° í†µí•©
- ì—¬ëŸ¬ íŒŒì¼ í˜•ì‹ í†µí•© ì²˜ë¦¬
- Redis ìºì‹±
- ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
- êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ìƒì„±

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ë°ì´í„° ì¶”ì¶œì´ ì™„ë£Œë˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”:

1. **Kafka ìŠ¤íŠ¸ë¦¬ë°**: ì¶”ì¶œëœ ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡
2. **Airflow íŒŒì´í”„ë¼ì¸**: ì •ê¸°ì ì¸ ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
3. **Redis ìºì‹±**: ì‹¤ì‹œê°„ ë°ì´í„° ìºì‹±
4. **ì›¹ ì„œë¹„ìŠ¤**: ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹œë³´ë“œ ê°œë°œ
5. **ë°°í¬**: Docker, Kubernetes, AWSë¥¼ í†µí•œ ë°°í¬

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

1. ë¡œê·¸ íŒŒì¼: `extraction.log`
2. Python ë²„ì „: 3.8 ì´ìƒ
3. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: `pip install -r requirements.txt`
4. ë°ì´í„° íŒŒì¼ ìœ„ì¹˜: `data/` í´ë”ì— ì›ë³¸ íŒŒì¼ë“¤ì´ ìˆì–´ì•¼ í•¨
