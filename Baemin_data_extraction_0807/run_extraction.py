#!/usr/bin/env python3
"""
ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë°°ë‹¬ì˜ë¯¼ì¡± ë¹„ì •í˜• ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import argparse
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data_extraction.data_extractor import DataExtractor
from data_extraction.text_analyzer import TextAnalyzer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def setup_environment():
    """í™˜ê²½ ì„¤ì •"""
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    directories = ['extracted_data', 'reports', 'visualizations']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    logger.info("í™˜ê²½ ì„¤ì • ì™„ë£Œ")


def run_data_extraction():
    """ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰"""
    logger.info("ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    
    # DataExtractor ì´ˆê¸°í™”
    extractor = DataExtractor()
    
    # Redis ì„¤ì • (ì„ íƒì‚¬í•­)
    try:
        extractor.setup_redis()
        logger.info("Redis ì—°ê²° ì„±ê³µ")
    except Exception as e:
        logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ëª¨ë“  ë°ì´í„° ì¶”ì¶œ
    results = extractor.extract_all_data()
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    summary = extractor.generate_summary_report(results)
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
    extractor.save_to_file(summary, "extraction_summary.json")
    
    logger.info("ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return results, summary


def run_text_analysis(results):
    """í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰"""
    logger.info("í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘")
    
    # TextAnalyzer ì´ˆê¸°í™”
    analyzer = TextAnalyzer(language='korean')
    
    # ë©”ì‹œì§€ ë°ì´í„° ë¶„ì„
    message_files = [
        'ë°°ë‹¬ì˜ë¯¼ì¡±_ë©”ì‹œì§€_ë°ì´í„°_25000ê±´.json',
        'ë°°ë‹¬ì˜ë¯¼ì¡±_JSON_ë°ì´í„°_25000ê±´.json'
    ]
    
    all_texts = []
    all_messages = []
    
    for filename in message_files:
        if filename in results and 'data' in results[filename]:
            data = results[filename]['data']
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        text = item.get('text', item.get('message', ''))
                        if text:
                            all_texts.append(text)
                            all_messages.append(item)
    
    if all_texts:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = analyzer.extract_keywords(all_texts, top_n=50)
        
        # ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„
        message_analysis = analyzer.analyze_message_patterns(all_messages)
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        try:
            wordcloud = analyzer.generate_wordcloud(
                all_texts, 
                'visualizations/wordcloud.png'
            )
            logger.info("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_results = {
            'keywords': keywords,
            'message_analysis': message_analysis,
            'total_texts': len(all_texts),
            'analysis_time': datetime.now().isoformat()
        }
        
        with open('reports/text_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = analyzer.generate_report(message_analysis)
        with open('reports/text_analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
        return analysis_results
    
    return None


def run_log_analysis(results):
    """ë¡œê·¸ ë°ì´í„° ë¶„ì„"""
    logger.info("ë¡œê·¸ ë¶„ì„ ì‹œì‘")
    
    log_file = 'ë°°ë‹¬ì˜ë¯¼ì¡±_ë¡œê·¸_ë°ì´í„°_25000ê±´.log'
    if log_file in results and 'data' in results[log_file]:
        log_data = results[log_file]['data']
        
        # ë¡œê·¸ ë ˆë²¨ë³„ ë¶„ì„
        level_counts = {}
        time_patterns = {}
        error_messages = []
        
        for log_entry in log_data:
            level = log_entry.get('level', 'UNKNOWN')
            level_counts[level] = level_counts.get(level, 0) + 1
            
            # ì‹œê°„ íŒ¨í„´ ë¶„ì„
            timestamp = log_entry.get('timestamp')
            if timestamp:
                try:
                    hour = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).hour
                    time_patterns[hour] = time_patterns.get(hour, 0) + 1
                except:
                    pass
            
            # ì—ëŸ¬ ë©”ì‹œì§€ ìˆ˜ì§‘
            if level in ['ERROR', 'CRITICAL']:
                error_messages.append(log_entry.get('message', ''))
        
        log_analysis = {
            'total_logs': len(log_data),
            'level_distribution': level_counts,
            'time_patterns': time_patterns,
            'error_count': len(error_messages),
            'top_errors': error_messages[:10]
        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        with open('reports/log_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(log_analysis, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info("ë¡œê·¸ ë¶„ì„ ì™„ë£Œ")
        return log_analysis
    
    return None


def generate_final_report(results, summary, text_analysis=None, log_analysis=None):
    """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
    logger.info("ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±")
    
    report = {
        'extraction_summary': summary,
        'text_analysis': text_analysis,
        'log_analysis': log_analysis,
        'generated_at': datetime.now().isoformat(),
        'file_statistics': {}
    }
    
    # íŒŒì¼ë³„ í†µê³„
    for filename, result in results.items():
        if 'error' not in result:
            if 'count' in result:
                report['file_statistics'][filename] = {
                    'type': result['type'],
                    'records': result['count']
                }
            elif 'shape' in result:
                report['file_statistics'][filename] = {
                    'type': result['type'],
                    'shape': result['shape']
                }
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    with open('reports/final_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
    text_report = []
    text_report.append("=" * 60)
    text_report.append("ë°°ë‹¬ì˜ë¯¼ì¡± ë°ì´í„° ì¶”ì¶œ ë° ë¶„ì„ ë¦¬í¬íŠ¸")
    text_report.append("=" * 60)
    text_report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    text_report.append("")
    
    text_report.append("ğŸ“Š ì¶”ì¶œ ìš”ì•½")
    text_report.append(f"- ì´ íŒŒì¼ ìˆ˜: {summary['total_files']}")
    text_report.append(f"- ì„±ê³µ: {summary['successful_extractions']}")
    text_report.append(f"- ì‹¤íŒ¨: {summary['failed_extractions']}")
    text_report.append(f"- ì´ ë ˆì½”ë“œ ìˆ˜: {summary['total_records']:,}")
    text_report.append("")
    
    text_report.append("ğŸ“ íŒŒì¼ë³„ í†µê³„")
    for filename, stats in report['file_statistics'].items():
        if 'records' in stats:
            text_report.append(f"- {filename}: {stats['records']:,}ê°œ ë ˆì½”ë“œ")
        elif 'shape' in stats:
            text_report.append(f"- {filename}: {stats['shape'][0]:,}í–‰ x {stats['shape'][1]}ì—´")
    text_report.append("")
    
    if text_analysis:
        text_report.append("ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼")
        text_report.append(f"- ë¶„ì„ëœ í…ìŠ¤íŠ¸: {text_analysis['total_texts']:,}ê°œ")
        text_report.append(f"- ì£¼ìš” í‚¤ì›Œë“œ ìˆ˜: {len(text_analysis['keywords'])}ê°œ")
        if 'message_analysis' in text_analysis:
            msg_analysis = text_analysis['message_analysis']
            text_report.append(f"- í‰ê·  ë©”ì‹œì§€ ê¸¸ì´: {msg_analysis.get('avg_message_length', 0):.1f}ì")
            text_report.append(f"- í‰ê·  ê°ì • ì ìˆ˜: {msg_analysis.get('avg_sentiment', 0):.3f}")
    text_report.append("")
    
    if log_analysis:
        text_report.append("ğŸ“‹ ë¡œê·¸ ë¶„ì„ ê²°ê³¼")
        text_report.append(f"- ì´ ë¡œê·¸ ìˆ˜: {log_analysis['total_logs']:,}ê°œ")
        text_report.append(f"- ì—ëŸ¬ ìˆ˜: {log_analysis['error_count']:,}ê°œ")
        text_report.append("- ë¡œê·¸ ë ˆë²¨ ë¶„í¬:")
        for level, count in log_analysis['level_distribution'].items():
            text_report.append(f"  * {level}: {count:,}ê°œ")
    text_report.append("")
    
    text_report.append("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„")
    text_report.append("1. ì¶”ì¶œëœ ë°ì´í„°ë¥¼ Kafkaë¡œ ìŠ¤íŠ¸ë¦¬ë°")
    text_report.append("2. Airflowë¥¼ í†µí•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
    text_report.append("3. Redisì— ì‹¤ì‹œê°„ ë°ì´í„° ìºì‹±")
    text_report.append("4. PostgreSQLì— êµ¬ì¡°í™”ëœ ë°ì´í„° ì €ì¥")
    text_report.append("5. ì›¹ ì„œë¹„ìŠ¤ ê°œë°œ ë° ë°°í¬")
    text_report.append("")
    
    text_report.append("=" * 60)
    
    with open('reports/final_report.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(text_report))
    
    logger.info("ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë°°ë‹¬ì˜ë¯¼ì¡± ë°ì´í„° ì¶”ì¶œ ë° ë¶„ì„')
    parser.add_argument('--skip-text-analysis', action='store_true', 
                       help='í…ìŠ¤íŠ¸ ë¶„ì„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-log-analysis', action='store_true', 
                       help='ë¡œê·¸ ë¶„ì„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--redis-host', default='localhost', 
                       help='Redis í˜¸ìŠ¤íŠ¸')
    parser.add_argument('--redis-port', type=int, default=6379, 
                       help='Redis í¬íŠ¸')
    
    args = parser.parse_args()
    
    try:
        # í™˜ê²½ ì„¤ì •
        setup_environment()
        
        # ë°ì´í„° ì¶”ì¶œ
        results, summary = run_data_extraction()
        
        # í…ìŠ¤íŠ¸ ë¶„ì„
        text_analysis = None
        if not args.skip_text_analysis:
            text_analysis = run_text_analysis(results)
        
        # ë¡œê·¸ ë¶„ì„
        log_analysis = None
        if not args.skip_log_analysis:
            log_analysis = run_log_analysis(results)
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        generate_final_report(results, summary, text_analysis, log_analysis)
        
        print("\n" + "=" * 50)
        print("âœ… ë°ì´í„° ì¶”ì¶œ ë° ë¶„ì„ ì™„ë£Œ!")
        print("=" * 50)
        print(f"ğŸ“Š ì´ íŒŒì¼ ìˆ˜: {summary['total_files']}")
        print(f"âœ… ì„±ê³µ: {summary['successful_extractions']}")
        print(f"âŒ ì‹¤íŒ¨: {summary['failed_extractions']}")
        print(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ ìˆ˜: {summary['total_records']:,}")
        print("\nğŸ“ ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:")
        print("- ì¶”ì¶œëœ ë°ì´í„°: extracted_data/")
        print("- ë¶„ì„ ë¦¬í¬íŠ¸: reports/")
        print("- ì‹œê°í™”: visualizations/")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: README.mdì˜ Phase 2ë¶€í„° ì§„í–‰í•˜ì„¸ìš”!")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
