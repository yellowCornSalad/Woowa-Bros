"""
í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ë„êµ¬
ë°°ë‹¬ì˜ë¯¼ì¡± ë©”ì‹œì§€ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import re
import json
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
from textblob import TextBlob
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')

logger = logging.getLogger(__name__)


class TextAnalyzer:
    """í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, language: str = 'korean'):
        """
        ì´ˆê¸°í™”
        
        Args:
            language: ë¶„ì„í•  ì–¸ì–´ ('korean', 'english')
        """
        self.language = language
        self.stop_words = set()
        self.lemmatizer = WordNetLemmatizer()
        
        # ì–¸ì–´ë³„ ì„¤ì •
        if language == 'english':
            self.stop_words = set(stopwords.words('english'))
        elif language == 'korean':
            # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì„¤ì •
            self.stop_words = {
                'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë•Œ', 'ê³³', 'ë§', 'ì¼',
                'ë˜', 'ë”', 'ë§', 'ì ', 'ê°€', 'ë‚˜', 'ë‹¤', 'ë¼', 'ë§ˆ', 'ë°”', 'ì‚¬',
                'ì•„', 'ì', 'ì°¨', 'ì¹´', 'íƒ€', 'íŒŒ', 'í•˜', 'ê±°', 'ë„ˆ', 'ë”', 'ëŸ¬',
                'ë¨¸', 'ë²„', 'ì„œ', 'ì–´', 'ì €', 'ê±°', 'ë„ˆ', 'ë”', 'ëŸ¬', 'ë¨¸', 'ë²„',
                'ì„œ', 'ì–´', 'ì €', 'ê³ ', 'ëŠ”', 'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì˜', 'ì—',
                'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ì„œ', 'ì—ê²Œ',
                'ê»˜ì„œ', 'í•œí…Œ', 'ì—ê²Œì„œ', 'ë¡œë¶€í„°', 'ë¡œì„œ', 'ë¡œì¨', 'ê°™ì´', 'ì²˜ëŸ¼',
                'ë§Œí¼', 'ë§Œì¹˜', 'ì¯¤', 'ì •ë„', 'ì¯¤', 'ì¯¤', 'ì¯¤', 'ì¯¤', 'ì¯¤'
            }
        
        logger.info(f"TextAnalyzer ì´ˆê¸°í™” ì™„ë£Œ - ì–¸ì–´: {language}")
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ì†Œë¬¸ì ë³€í™˜ (ì˜ì–´ì˜ ê²½ìš°)
        if self.language == 'english':
            text = text.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œêµ­ì–´ì˜ ê²½ìš° ì¼ë¶€ ë³´ì¡´)
        if self.language == 'korean':
            text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        else:
            text = re.sub(r'[^\w\s]', ' ', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”"""
        if self.language == 'korean':
            # í•œêµ­ì–´ í† í°í™”
            tokens = jieba.lcut(text)
        else:
            # ì˜ì–´ í† í°í™”
            tokens = word_tokenize(text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 1]
        
        return tokens
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """ê°ì • ë¶„ì„"""
        try:
            if self.language == 'english':
                blob = TextBlob(text)
                return {
                    'polarity': blob.sentiment.polarity,  # -1 (ë¶€ì •) ~ 1 (ê¸ì •)
                    'subjectivity': blob.sentiment.subjectivity  # 0 (ê°ê´€) ~ 1 (ì£¼ê´€)
                }
            else:
                # í•œêµ­ì–´ ê°ì • ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
                positive_words = ['ì¢‹', 'ë§›ìˆ', 'í›Œë¥­', 'ì™„ë²½', 'ìµœê³ ', 'ê°ì‚¬', 'ë§Œì¡±']
                negative_words = ['ë‚˜ì˜', 'ë³„ë¡œ', 'ìµœì•…', 'ë¶ˆë§Œ', 'ì‹¤ë§', 'í™”ë‚˜', 'ì§œì¦']
                
                text_lower = text.lower()
                positive_count = sum(1 for word in positive_words if word in text_lower)
                negative_count = sum(1 for word in negative_words if word in text_lower)
                
                total_words = len(text.split())
                if total_words == 0:
                    return {'polarity': 0, 'subjectivity': 0}
                
                polarity = (positive_count - negative_count) / total_words
                subjectivity = (positive_count + negative_count) / total_words
                
                return {
                    'polarity': max(-1, min(1, polarity)),
                    'subjectivity': max(0, min(1, subjectivity))
                }
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'polarity': 0, 'subjectivity': 0}
    
    def extract_keywords(self, texts: List[str], top_n: int = 20) -> List[Tuple[str, int]]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_tokens = []
        
        for text in texts:
            processed_text = self.preprocess_text(text)
            tokens = self.tokenize_text(processed_text)
            all_tokens.extend(tokens)
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(all_tokens)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        return word_freq.most_common(top_n)
    
    def analyze_message_patterns(self, messages: List[Dict]) -> Dict[str, Any]:
        """ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„"""
        analysis = {
            'total_messages': len(messages),
            'message_lengths': [],
            'sentiment_scores': [],
            'time_patterns': defaultdict(int),
            'user_patterns': defaultdict(int),
            'keyword_frequency': Counter(),
            'response_times': []
        }
        
        for message in messages:
            # ë©”ì‹œì§€ ê¸¸ì´
            text = message.get('text', '')
            analysis['message_lengths'].append(len(text))
            
            # ê°ì • ë¶„ì„
            sentiment = self.analyze_sentiment(text)
            analysis['sentiment_scores'].append(sentiment['polarity'])
            
            # ì‹œê°„ íŒ¨í„´
            timestamp = message.get('timestamp', '')
            if timestamp:
                try:
                    hour = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).hour
                    analysis['time_patterns'][hour] += 1
                except:
                    pass
            
            # ì‚¬ìš©ì íŒ¨í„´
            user_id = message.get('user_id', 'unknown')
            analysis['user_patterns'][user_id] += 1
            
            # í‚¤ì›Œë“œ ë¹ˆë„
            tokens = self.tokenize_text(self.preprocess_text(text))
            analysis['keyword_frequency'].update(tokens)
        
        # í†µê³„ ê³„ì‚°
        analysis['avg_message_length'] = np.mean(analysis['message_lengths'])
        analysis['avg_sentiment'] = np.mean(analysis['sentiment_scores'])
        analysis['top_keywords'] = analysis['keyword_frequency'].most_common(20)
        analysis['top_users'] = dict(analysis['user_patterns'].most_common(10))
        
        return analysis
    
    def generate_wordcloud(self, texts: List[str], output_path: str = None) -> WordCloud:
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = ' '.join(texts)
        
        # í•œêµ­ì–´ í°íŠ¸ ì„¤ì •
        if self.language == 'korean':
            font_path = '/System/Library/Fonts/AppleGothic.ttf'  # macOS
            # Windows: 'C:/Windows/Fonts/malgun.ttf'
            # Linux: '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
        else:
            font_path = None
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(
            font_path=font_path,
            width=800,
            height=400,
            background_color='white',
            max_words=100,
            colormap='viridis'
        ).generate(combined_text)
        
        # ì €ì¥
        if output_path:
            wordcloud.to_file(output_path)
            logger.info(f"ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return wordcloud
    
    def analyze_customer_feedback(self, feedback_data: List[Dict]) -> Dict[str, Any]:
        """ê³ ê° í”¼ë“œë°± ë¶„ì„"""
        analysis = {
            'total_feedback': len(feedback_data),
            'rating_distribution': defaultdict(int),
            'sentiment_by_rating': defaultdict(list),
            'common_issues': Counter(),
            'positive_keywords': Counter(),
            'negative_keywords': Counter(),
            'response_time_analysis': {},
            'category_analysis': defaultdict(list)
        }
        
        for feedback in feedback_data:
            rating = feedback.get('rating', 0)
            text = feedback.get('text', '')
            category = feedback.get('category', 'general')
            
            # í‰ì  ë¶„í¬
            analysis['rating_distribution'][rating] += 1
            
            # í‰ì ë³„ ê°ì • ë¶„ì„
            sentiment = self.analyze_sentiment(text)
            analysis['sentiment_by_rating'][rating].append(sentiment['polarity'])
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
            analysis['category_analysis'][category].append({
                'rating': rating,
                'sentiment': sentiment['polarity'],
                'text': text
            })
            
            # í‚¤ì›Œë“œ ë¶„ì„
            tokens = self.tokenize_text(self.preprocess_text(text))
            if rating >= 4:
                analysis['positive_keywords'].update(tokens)
            elif rating <= 2:
                analysis['negative_keywords'].update(tokens)
        
        # í†µê³„ ê³„ì‚°
        for rating in analysis['sentiment_by_rating']:
            analysis['sentiment_by_rating'][rating] = np.mean(analysis['sentiment_by_rating'][rating])
        
        analysis['top_positive_keywords'] = analysis['positive_keywords'].most_common(10)
        analysis['top_negative_keywords'] = analysis['negative_keywords'].most_common(10)
        
        return analysis
    
    def detect_anomalies(self, texts: List[str], threshold: float = 2.0) -> List[int]:
        """ì´ìƒ í…ìŠ¤íŠ¸ íƒì§€"""
        anomalies = []
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì´ìƒ íƒì§€
        lengths = [len(text) for text in texts]
        mean_length = np.mean(lengths)
        std_length = np.std(lengths)
        
        for i, length in enumerate(lengths):
            z_score = abs(length - mean_length) / std_length
            if z_score > threshold:
                anomalies.append(i)
        
        return anomalies
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """ê°œì²´ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)"""
        entities = {
            'restaurants': [],
            'food_items': [],
            'locations': [],
            'times': [],
            'prices': []
        }
        
        # ìŒì‹ì ëª… íŒ¨í„´
        restaurant_patterns = [
            r'([ê°€-í£]+[ì§‘|ì |ë‹¹|ê´€])',
            r'([ê°€-í£]+[ì¹˜í‚¨|í”¼ì|í–„ë²„ê±°|ì¹´í˜])'
        ]
        
        # ìŒì‹ëª… íŒ¨í„´
        food_patterns = [
            r'([ê°€-í£]+[ì¹˜í‚¨|í”¼ì|í–„ë²„ê±°|ìŠ¤í…Œì´í¬|íŒŒìŠ¤íƒ€|ìƒëŸ¬ë“œ])',
            r'([ê°€-í£]+[ê¹€ì¹˜|ëœì¥|ìˆœë‘ë¶€|ê°ˆë¹„|ì‚¼ê²¹ì‚´])'
        ]
        
        # ê°€ê²© íŒ¨í„´
        price_pattern = r'(\d{1,3}(?:,\d{3})*ì›)'
        
        # ì‹œê°„ íŒ¨í„´
        time_pattern = r'(\d{1,2}:\d{2})'
        
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in restaurant_patterns:
            matches = re.findall(pattern, text)
            entities['restaurants'].extend(matches)
        
        for pattern in food_patterns:
            matches = re.findall(pattern, text)
            entities['food_items'].extend(matches)
        
        price_matches = re.findall(price_pattern, text)
        entities['prices'].extend(price_matches)
        
        time_matches = re.findall(time_pattern, text)
        entities['times'].extend(time_matches)
        
        return entities
    
    def generate_report(self, analysis_results: Dict[str, Any]) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 50)
        report.append("í…ìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 50)
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ê¸°ë³¸ í†µê³„
        if 'total_messages' in analysis_results:
            report.append("ğŸ“Š ê¸°ë³¸ í†µê³„")
            report.append(f"- ì´ ë©”ì‹œì§€ ìˆ˜: {analysis_results['total_messages']:,}ê°œ")
            report.append(f"- í‰ê·  ë©”ì‹œì§€ ê¸¸ì´: {analysis_results.get('avg_message_length', 0):.1f}ì")
            report.append(f"- í‰ê·  ê°ì • ì ìˆ˜: {analysis_results.get('avg_sentiment', 0):.3f}")
            report.append("")
        
        # í‚¤ì›Œë“œ ë¶„ì„
        if 'top_keywords' in analysis_results:
            report.append("ğŸ” ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)")
            for i, (keyword, count) in enumerate(analysis_results['top_keywords'][:10], 1):
                report.append(f"{i:2d}. {keyword}: {count:,}íšŒ")
            report.append("")
        
        # ì‹œê°„ íŒ¨í„´
        if 'time_patterns' in analysis_results:
            report.append("â° ì‹œê°„ëŒ€ë³„ ë©”ì‹œì§€ ë¶„í¬")
            for hour in sorted(analysis_results['time_patterns'].keys()):
                count = analysis_results['time_patterns'][hour]
                report.append(f"- {hour:02d}ì‹œ: {count:,}ê°œ")
            report.append("")
        
        # ì‚¬ìš©ì íŒ¨í„´
        if 'top_users' in analysis_results:
            report.append("ğŸ‘¥ í™œì„± ì‚¬ìš©ì (ìƒìœ„ 5ëª…)")
            for i, (user_id, count) in enumerate(analysis_results['top_users'].items(), 1):
                report.append(f"{i}. ì‚¬ìš©ì {user_id}: {count:,}ê°œ ë©”ì‹œì§€")
            report.append("")
        
        return "\n".join(report)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # TextAnalyzer ì´ˆê¸°í™”
    analyzer = TextAnalyzer(language='korean')
    
    # ì˜ˆì‹œ ë°ì´í„°
    sample_texts = [
        "ë§›ìˆëŠ” ì¹˜í‚¨ ë°°ë‹¬í•´ì£¼ì„¸ìš”!",
        "í”¼ì ì£¼ë¬¸í–ˆëŠ”ë° ë„ˆë¬´ ëŠ¦ê²Œ ì™”ì–´ìš”",
        "ê°ì‚¬í•©ë‹ˆë‹¤! ì •ë§ ë§›ìˆì—ˆì–´ìš”",
        "ë°°ë‹¬ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ìš”",
        "ìŒì‹ì´ ë§›ìˆê³  ì„œë¹„ìŠ¤ë„ ì¢‹ì•„ìš”"
    ]
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = analyzer.extract_keywords(sample_texts)
    print("ì£¼ìš” í‚¤ì›Œë“œ:")
    for keyword, count in keywords[:10]:
        print(f"- {keyword}: {count}íšŒ")
    
    # ê°ì • ë¶„ì„
    print("\nê°ì • ë¶„ì„:")
    for text in sample_texts:
        sentiment = analyzer.analyze_sentiment(text)
        print(f"- '{text}': {sentiment['polarity']:.3f}")
    
    # ê°œì²´ëª… ì¶”ì¶œ
    print("\nê°œì²´ëª… ì¶”ì¶œ:")
    for text in sample_texts:
        entities = analyzer.extract_entities(text)
        print(f"- '{text}':")
        for entity_type, items in entities.items():
            if items:
                print(f"  {entity_type}: {items}")


if __name__ == "__main__":
    main()
